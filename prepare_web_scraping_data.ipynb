{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import acquire as a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/forrestmccrosky/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download the first time\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire \n",
    "\n",
    "First lets get some text to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the http request and turn the response into a beautiful soup object\n",
    "\n",
    "headers = headers = {'User-Agent': 'Codeup Data Science'}\n",
    "\n",
    "response = requests.get('https://codeup.com/codeups-data-science-career-accelerator-is-here/', \n",
    "                       headers = headers)\n",
    "html = response.text\n",
    "soup = bs4.BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting our article content into the article variable\n",
    "\n",
    "article = soup.find('div', class_ = 'jupiterx-post-content')\n",
    "\n",
    "original = article.text[:300] ## looking at our article content\n",
    "string_function = article.text[:300] ## making function test variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    " - Lowercase everything\n",
    " - Normalize unicode characters\n",
    " - Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The rumors are true! The time has arrived. Codeup has officially opened applications to our new Data Science career accelerator, with only 25 seats available! This immersive program is one of a kind in San Antonio, and will help you land a job in\\xa0Glassdoor’s #1 Best Job in America.\\nData Science is a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original ## now we have our sample string to perform the exercises with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumors are true! the time has arrived. codeup has officially opened applications to our new data science career accelerator, with only 25 seats available! this immersive program is one of a kind in san antonio, and will help you land a job in\\xa0glassdoor’s #1 best job in america.\\ndata science is a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = original.lower() ## lowercasing all the a to z characters in string\n",
    "\n",
    "string ## <-- quality assurance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumors are true! the time has arrived. codeup has officially opened applications to our new data science career accelerator, with only 25 seats available! this immersive program is one of a kind in san antonio, and will help you land a job in glassdoors #1 best job in america.\\ndata science is a'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## normalizing the unicode characters\n",
    "\n",
    "string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "string ## <-- quality assurance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumors are true the time has arrived codeup has officially opened applications to our new data science career accelerator with only 25 seats available this immersive program is one of a kind in san antonio and will help you land a job in glassdoors 1 best job in america\\ndata science is a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "\n",
    "string ## <-- quality assurance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and performs some basic cleaning.\n",
    "    \n",
    "    Converts all alphabet characters to lowercase\n",
    "    Normalize unicode characters\n",
    "    recplace anything that is not a letter, number, whitespace or a single quote\n",
    "    '''\n",
    "    ## convert all alphabet characters to lowercase\n",
    "    string = string.lower()\n",
    "    \n",
    "    ## normalizing the unicode characters\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    \n",
    "    ## removing special characters\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumors are true the time has arrived codeup has officially opened applications to our new data science career accelerator with only 25 seats available this immersive program is one of a kind in san antonio and will help you land a job in glassdoors 1 best job in america\\ndata science is a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_test = basic_clean(string_function) ## testing our function\n",
    "\n",
    "string_test ## <-- quality assurance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rumors are true the time has arrived codeup has officially opened applications to our new data science career accelerator with only 25 seats available this immersive program is one of a kind in san antonio and will help you land a job in glassdoors 1 best job in america\n",
      "data science is a\n"
     ]
    }
   ],
   "source": [
    "#create the tokenizer\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "#use the tokenizer\n",
    "print(tokenizer.tokenize(string_test, return_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function is designed to take in a string and tokenize all \n",
    "    the words in the string\n",
    "    '''\n",
    "    #create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    #use the tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumors are true the time has arrived codeup has officially opened applications to our new data science career accelerator with only 25 seats available this immersive program is one of a kind in san antonio and will help you land a job in glassdoors 1 best job in america\\ndata science is a'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = tokenize(string_test) ## doing our function thing\n",
    "\n",
    "string ## <-- quality assurance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumor are true the time ha arriv codeup ha offici open applic to our new data scienc career acceler with onli 25 seat avail thi immers program is one of a kind in san antonio and will help you land a job in glassdoor 1 best job in america data scienc is a'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the nltk stemmer object, then use it\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "stems = [ps.stem(word) for word in string.split()]\n",
    "strings_stemmed = ' '.join(stems)\n",
    "\n",
    "strings_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function is designed to take in a string as input and stems the words \n",
    "    in the string \n",
    "    '''\n",
    "    ## create the nltk stemmer object, then use it\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    ## used list comprehension on our split string to stem the words\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    string_stemmed = ' '.join(stems)\n",
    "    \n",
    "    return string_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumor are true the time ha arriv codeup ha offici open applic to our new data scienc career acceler with onli 25 seat avail thi immers program is one of a kind in san antonio and will help you land a job in glassdoor 1 best job in america data scienc is a'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = stem(string) ## using our function to stem the words\n",
    " \n",
    "string ## <-- quality assurance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "\n",
    "Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function is designed to take in a string as input and lemmatize the words \n",
    "    in the string \n",
    "    '''\n",
    "    ## create the nltk lemmatizer object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    ## use the lemmatizer object\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    string_lemmatized = ' '.join(lemmas)\n",
    "    \n",
    "    return string_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rumor are true the time ha arriv codeup ha offici open applic to our new data scienc career acceler with onli 25 seat avail thi immers program is one of a kind in san antonio and will help you land a job in glassdoor 1 best job in america data scienc is a'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = lemmatize(string) ## using our function\n",
    "\n",
    "string ## <-- quality assurance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: \n",
    "\n",
    "Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function is designed to take in a string as input and return the words \n",
    "    in the string after removing all the stopwords\n",
    "    '''\n",
    "    \n",
    "    ## creating our stop word list from the english word list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    ## removing excluded words\n",
    "    for word in exclude_words:\n",
    "        stopword_list.remove(word)\n",
    "    \n",
    "    ## adding extra words\n",
    "    for word in extra_words:\n",
    "        stopword_list.append(word)\n",
    "    \n",
    "    ## creating a list of words from the original inputted string after splitting them\n",
    "    words = string.split()\n",
    "    \n",
    "    ## removing stop words and setting it equal to filtered_words\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    \n",
    "    print('Removed {} stopwords'.format(len(words) - len(filtered_words)))\n",
    "    print('---')\n",
    "    \n",
    "    ## recreating our string without stopwords\n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return article_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 18 stopwords\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rumor true time ha arriv codeup ha offici open applic new data scienc career acceler onli 25 seat avail thi immers program one kind san antonio help land job glassdoor 1 best job america data scienc'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = remove_stopwords(string) ## using our function to remove stop words\n",
    "\n",
    "string ## <-- quality assurance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
